{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagenes/ia.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Regresión logística y regularización\n",
    "\n",
    "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/).\n",
    "\n",
    "En esta libreta vamos a revisar el algoritmo básco de la regresión logística y los principios básicos de regularización. Si bien es una primera aprximación, espero que se pueda ilustrar en esta libreta tanto el uso básico de la regresión logística como el efecto de la regularización.\n",
    "\n",
    "Empecemos por inicializar los modulos que vamos a requerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image  # Esto es para desplegar imágenes en la libreta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Función logística, función de error *en muestra* y gradiente de la función de error *en muestra*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función logística está dada por \n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}},\n",
    "$$\n",
    "\n",
    "la cual es importante que podamos calcular en forma vectorial. Si bien el calculo es de una sola linea, el uso de estas funciones auxiliares facilitan la legibilidad del código.\n",
    "\n",
    "#### Ejercicio 1: Desarrolla la función logística, la cual se calcule para todos los elementos de un ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistica(z):\n",
    "    \"\"\"\n",
    "    Calcula la función logística para cada elemento de z\n",
    "    \n",
    "    @param z: un ndarray\n",
    "    @return: un ndarray de las mismas dimensiones que z\n",
    "    \"\"\"\n",
    "    # Introduce código aqui (una linea de código)\n",
    "    #---------------------------------------------------\n",
    "    return (1/(1+np.exp(-z)))\n",
    "    #---------------------------------------------------\n",
    "    \n",
    "# prueba que efectivamente funciona la función implementada\n",
    "# si el assert es falso regresa un error de aserción (el testunit de los pobres)\n",
    "assert (np.abs(logistica(np.array([-1, 0, 1])) - np.array([ 0.26894142, 0.5, 0.73105858]))).sum() < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender que hace la función logística, vamos a graficarla (sin clavarnos en los valores exactos) en el intervalo [-5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbUAAAE0CAYAAACrRq2gAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEUIIUQwqaoQQQhSDihohhBDF+H9k6HQ0mY96UwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2be49f44cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 100)\n",
    "with plt.xkcd():\n",
    "    plt.plot( z, logistica(z))\n",
    "    plt.title(u'Función logística', fontsize=20)\n",
    "    plt.xlabel(r'$z$', fontsize=20)\n",
    "    plt.ylabel(r'$\\frac{1}{1 + \\exp(-z)}$', fontsize=26)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez establecida la función logística, vamos a implementar la función de error en muestra *sin regularizar* que se utiliza típicamente en clasificación binaria, la cual está dada por\n",
    "\n",
    "$$\n",
    "E_{in}(y, \\hat y ) = -\\frac{1}{T} \\sum_{i=1}^T \\left[ y^{(i)}\\log(\\hat{y}^{(i)}) + (1 - y^{(i)})\\log(1 - \\hat{y}^{(i)})\\right],\n",
    "$$\n",
    "\n",
    "donde \n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = g(\\omega^T x_e^{(i)}),\n",
    "$$\n",
    "\n",
    "las cuales fueron ecuaciones revisadas en clase.\n",
    "\n",
    "#### Ejercicio 2: Implementa la función de error en muestra para un conjunto de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_muestra(w, x, y):\n",
    "    \"\"\"\n",
    "    Calcula el costo de una w dada para el conjunto dee entrenamiento dado por y y x\n",
    "    \n",
    "    @param w: un ndarray de dimensión (n + 1, 1) \n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "    @param y: un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    \n",
    "    @return: un flotante con el costo\n",
    "    \n",
    "    \"\"\" \n",
    "    T = x.shape[0]\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    # Agregua aqui tu código\n",
    "    y_gorrito=logistica(x.dot(w))\n",
    "    return ((-np.sum(y*np.log(y_gorrito)+(1-y)*np.log(1-y_gorrito)))/T)\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "# Otra vez el testunit del pobre (ya lo calcule yo, pero puedes hacerlo a mano para estar seguro)\n",
    "w = np.ones((2,1))\n",
    "\n",
    "x = np.array([[1, 10],\n",
    "              [1, -5]])\n",
    "\n",
    "y1 = np.array([[1],\n",
    "               [0]])\n",
    "\n",
    "y2 = np.array([[0],\n",
    "               [1]])\n",
    "\n",
    "y3 = np.array([[0],\n",
    "               [0]])\n",
    "\n",
    "y4 = np.array([[1],\n",
    "               [1]])\n",
    "\n",
    "assert abs(error_muestra(w, x, y1) - 0.01) < 1e-2\n",
    "assert abs(error_muestra(w, x, y2) - 7.5) < 1e-2\n",
    "assert abs(error_muestra(w, x, y3) - 5.5) < 1e-2\n",
    "assert abs(error_muestra(w, x, y4) - 2.0) < 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera, para poder implementar las funciones de aprendizaje, vamos a implementar el gradiente de la función de error respecto a $\\omega$, el cuál es (como lo vimos en clase) el siguiente:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E_{in}}{\\partial \\omega_j} = -\\frac{1}{T} \\sum_{i=1}^T \\left(y^{(i)} - g(\\omega^T x_e^{(i)})\\right)x_j^{(i)} \n",
    "$$\n",
    "\n",
    "y a partir de las ecuaciones individuales de puede obtener $\\nabla_\\omega E_{in}$. Como podemos ver, $\\left(y^{(i)} - g(\\omega^T x_e^{(i)})\\right)$ es un escalar, y es el mismo para todos los atrinutos $x^{(i)}_j$, por lo que el gradiente se puede expresar como\n",
    "\n",
    "$$\n",
    "\\nabla_\\omega E_{in} = -\\frac{1}{T} \\sum_{i=1}^T \\left(y^{(i)} - g(\\omega^T x_e^{(i)})\\right)x^{(i)}.\n",
    "$$\n",
    "\n",
    "\n",
    "Asumimamos entonces que $X = (x^{(1)}, x^{(2)}, \\ldots, x^{(T)})^T$ (esto es, el $i$--ésimo renglón representa una situación u objeto, mientras que la $j$--ésima columna representa los valores que toman los diferentes objetos de un atributo), y $Y = (y^{(1)}, y^{(1)}, \\ldots, y^{(T)})^T$ (un vector columna con los valores de salida del conjunto $X$). El error en muestra se puede calcular (a partir de la ecuación anterior) como:\n",
    "\n",
    "$$\n",
    "\\nabla_\\omega E_{in} = -\\frac{1}{T} X^T \\cdot (Y - g(X_e \\cdot w)),\n",
    "$$\n",
    "donde $X_e$ es la matriz $X$ agregandole al inicio una columna con unos (esto es, el atributo $0$ o valor constante.\n",
    "\n",
    "#### Ejercicio 3: Implementa (con operaciones matriciales) el calculo del gradiente de la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradiente(w, X, Y):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente de la función de pérdida para clasificación binaria, \n",
    "    utilizando una neurona logística, para una theta y conociendo un conjunto de aprendizaje.\n",
    "    \n",
    "    @param w: un ndarray de dimensión (n + 1, 1) \n",
    "    @param X: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "    @param Y: un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    \n",
    "    @return: un ndarray de mismas dimensiones que w\n",
    "    \n",
    "    \"\"\"\n",
    "    T = X.shape[0]\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    # Agregua aqui tu código\n",
    "    return ((-X.T.dot(Y-logistica(X.dot(w))))/T)\n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "# Otra vez el testunit del pobre (ya lo calcule yo, pero puedes hacerlo a mano para estar seguro)\n",
    "w = np.ones((2, 1))\n",
    "\n",
    "x = np.array([[1, 10],\n",
    "              [1, -5]])\n",
    "\n",
    "y1 = np.array([[1],\n",
    "               [0]])\n",
    "\n",
    "y2 = np.array([[0],\n",
    "               [1]])\n",
    "\n",
    "y3 = np.array([[0],\n",
    "               [0]])\n",
    "\n",
    "y4 = np.array([[1],\n",
    "               [1]])\n",
    "\n",
    "assert abs(0.00898475 - gradiente(w, x, y1)[0]) < 1e-4\n",
    "assert abs(7.45495097 - gradiente(w, x, y2)[1]) < 1e-4 \n",
    "assert abs(4.95495097 - gradiente(w, x, y3)[1]) < 1e-4 \n",
    "assert abs(-0.49101525 - gradiente(w, x, y4)[0]) < 1e-4     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regresión logística en acción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a desarrollar las funciones necesarias para realizar el entrenamiento y encontrar la mejor $\\omega$ de acuero a la función de costos y un conjunto de datos de aprendizaje.\n",
    "\n",
    "Para este problema, vamos a utilizar una base de datos sintética proveniente del curso de [Andrew Ng](www.andrewng.org/) que se encuentra en [Coursera](https://www.coursera.org). Supongamos que pertenecemos al departamente de servicios escolares de la UNISON y vamos a modificar el procedimiento de admisión. En lugar de utilizar un solo exámen (EXCOBA) y la información del cardex de la preparatoria, hemos decidido aplicar dos exámenes, uno sicométrico y otro de habilidades estudiantiles. Dichos exámenes se han aplicado el último año aunque no fueron utilizados como criterio. Así, tenemos un historial entre estudiantes aceptados y resultados de los dos exámenes. El objetivo es hacer un método de regresión que nos permita hacer la admisión a la UNISON tomando en cuenta únicamente los dos exámenes y simplificar el proceso. *Recuerda que esto no es verdad, es solo un ejercicio*.\n",
    "\n",
    "Bien, los datos se encuentran en el archivo `admision.txt` el cual se encuentra en formato `cvs` (osea los valores de las columnas separados por comas. Vamos a leer los datos y graficar la información para entender un poco los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datos = np.loadtxt('datos/admision.txt', comments='%', delimiter=',')\n",
    "\n",
    "x, y = datos[:,0:-1], datos[:,-1:] \n",
    "x = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "\n",
    "plt.plot(x[y.ravel() == 1, 1], x[y.ravel() == 1, 2], 'sr', label='aceptados') \n",
    "plt.plot(x[y.ravel() == 0, 1], x[y.ravel() == 0, 2], 'ob', label='rechazados')\n",
    "plt.title(u'Ejemplo sintético para regresión logística')\n",
    "plt.xlabel(u'Calificación del primer examen')\n",
    "plt.ylabel(u'Calificación del segundo examen')\n",
    "plt.axis([20, 100, 20, 100])\n",
    "plt.legend(loc=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vistos los datos un clasificador lineal podría ser una buena solución. Ahora vamos a implementar el método de descenso de gradiente.\n",
    "\n",
    "Para esto recuerda que el método de aprendizaje por descenso de gradiente se basa en actualizar la fórmulación:\n",
    "\n",
    "$$\n",
    "\\omega \\leftarrow \\omega - \\alpha \\nabla_\\omega E_{in} \n",
    "$$\n",
    "\n",
    "#### Ejercicio 4: Implementa el descenso de gradiente para el problema de regresión logística en modo batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def descenso(x, y, epsilon, tol=1e-4, max_iter=int(1e4), historial=False):\n",
    "    \"\"\"\n",
    "    Descenso de gradiente por lotes para resolver el problema de regresión logística con un conjunto de aprendizaje\n",
    "\n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "    \n",
    "    @param y: un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    \n",
    "    @param epsilon: Un flotante (típicamente pequeño) con la tasa de aprendizaje\n",
    "    \n",
    "    @param tol: Un flotante pequeño como criterio de paro. Por default 1e-4\n",
    "    \n",
    "    @param max_iter: Máximo numero de iteraciones. Por default 1e4\n",
    "    \n",
    "    @param historial: Un booleano para saber si guardamos el historial de la función de pérdida o no\n",
    "    \n",
    "    @return: w, error_hist donde w es ndarray de dimensión (n + 1, 1) y error_hist es un\n",
    "             ndarray de dimensión (max_iter,) con el valor de la función de error en muestra en cada iteración. \n",
    "             Si historial == True, entonces perdida_hist = None.\n",
    "             \n",
    "    \"\"\"\n",
    "    T, n = x.shape[0], x.shape[1] - 1\n",
    "    \n",
    "    w = np.zeros((n + 1, 1))\n",
    "    error_hist = np.zeros(max_iter) if historial else None\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        #--------------------------------------------------------------\n",
    "        # Agregar aqui tu código\n",
    "        #\n",
    "        # Recuerda utilizar las funciones que ya has desarrollado\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #--------------------------------------------------------------\n",
    "\n",
    "    return w, error_hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar la función de aprendizaje, vamos a aplicarla a nuestro problema de admisión. Primero tenemos que hacer una exploración para encontrar el mejor valor de $\\epsilon$. Así que utilizamos el código de abajo para ajustar $\\epsilon$ visualmente. \n",
    "\n",
    "Busca el valor mas grande de $\\epsilon$ tal que el método no se vuelva inestable, te recomiendo que utilices la estratégia de probar con `1e-4`, `5e-4`, `1e-3`, `5e-3`, `1e-2`, `5e-2`, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-4\n",
    "mi = 50\n",
    "_, error_hist = descenso(x, y, epsilon, tol=1e-4, max_iter=mi, historial=True)\n",
    "\n",
    "plt.plot(np.arange(mi), error_hist)\n",
    "plt.title(r'Evolucion del valor de $E_{in}$ en las primeras iteraciones con $\\epsilon$ = ' + str(epsilon))\n",
    "plt.xlabel('error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez encontrado el mejor $\\epsilon$, entonces podemos calcular $\\omega$ (esto va a tardar algo), recuerda que el costo final debe de ser lo más cercano a 0 posible, así que agrega cuantas iteraciones sean necesarias: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, _ = descenso(x, y, epsilon, max_iter = int(1e6))\n",
    "print(\"Los pesos obtenidos son: \\n{}\".format(w_lotes))\n",
    "print(\"El valor final de la función de error en muestra es: {}\".format(error_muestra(w_lotes, x, y))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método nos devuelve $\\omega$, pero esto no es suficiente para decir que tenemos un clasificador, ya que un método de clasificación se compone de dos métodos, uno para **aprender** y otro para **predecir**. Recuerda que para realizar la predicción tenemos que decidir cual es el umbral por el cual consideramos que la clase estimada es 1 (la clase distinguida)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 5: Desarrolla una función de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predictor(w, x):\n",
    "    \"\"\"\n",
    "    Predice los valores de y_hat (que solo pueden ser 0 o 1), utilizando el criterio MAP.\n",
    "    \n",
    "    @param w: un ndarray de dimensión (n + 1, 1)\n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "\n",
    "    @return: y_hat un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    \"\"\"\n",
    "    #-------------------------------------------------------------------------------------\n",
    "    # Agrega aqui tu código sin utilizar la función logística\n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------------------\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Que tan bueno es este clasificador? ¿Es que implementamos bien el método?\n",
    "\n",
    "Vamos a contestar esto por partes. Primero, vamos a graficar los mismos datos pero vamos a agregar la superficie de separación, la cual en este caso sabemos que es una linea recta. Como sabemos el criterio para decidir si un punto pertenece a la clase 1 o cero es si el valor de $\\omega^T x_e^{(i)} \\ge 0$, por lo que la frontera entre la región donde se escoge una clase de otra se encuentra en:\n",
    "\n",
    "$$\n",
    "0 = \\omega_0 + \\omega_1 x_1  + \\omega_2 x_2,\n",
    "$$\n",
    "\n",
    "y despejando:\n",
    "\n",
    "$$\n",
    "x_2 = -\\frac{\\omega_0}{\\omega_2} -\\frac{\\omega_1}{\\omega_2}x_1\n",
    "$$\n",
    "\n",
    "son los pares $(x_1, x_2)$ de valores en la forntera. Al ser estos (en este caso) una linea recta solo necesitamos dos para graficar la superficie de separación. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1_frontera = np.array([20, 100]) #Los valores mínimo y máximo que tenemos en la gráfica de puntos\n",
    "x2_frontera = -(w[0] / w[2]) - (w[1] / w[2]) * x1_frontera\n",
    "\n",
    "plt.plot(x[y.ravel() == 1, 1], x[y.ravel() == 1, 2], 'sr', label='aceptados') \n",
    "plt.plot(x[y.ravel() == 0, 1], x[y.ravel() == 0, 2], 'ob', label='rechazados')\n",
    "plt.plot(x1_frontera, x2_frontera, 'm')\n",
    "plt.title(u'Ejemplo sintético para regresión logística')\n",
    "plt.xlabel(u'Calificación del primer examen')\n",
    "plt.ylabel(u'Calificación del segundo examen')\n",
    "plt.axis([20, 100, 20, 100])\n",
    "plt.legend(loc=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para que tengas una idea de lo que debería de salir, anexo una figura obtenida con el código que yo hice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename='ejemplo_logistica_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generando atributos polinomiales\n",
    "\n",
    "Como podemos ver en las gráficas de arriba, parece ser que la regresión logística aceptaría a algunos estudiantes rechazados y rechazaría a algunos que si fueron en realidad aceptados. En todo método de clasificación hay un grado de error, y eso es parte del poder de generalización de los métodos. \n",
    "\n",
    "Sin embargo, una simple inspección visual muestra que, posiblemente, la regresión lineal no es la mejor solución, ya que la frontera entre las dos clases parece ser más bien una curva.\n",
    "\n",
    "¿Que tal si probamos con un clasificador cuadrático? Un clasificador cuadrático no es más que la regresión logística pero a la que se le agregan todos los atributos que sean una combinación de dos de los atributos. \n",
    "\n",
    "Por ejemplo, si un objeto $x_e = (1, x_1, x_2, x_3)^T$ se aumenta con todas sus componentes cuadráticas, entonces tenemos los atributos\n",
    "\n",
    "$$\n",
    "\\phi_2(x_e) = (1, x_1, x_2, x_3, x_1 x_2, x_1 x_3, x_2 x_3, x_1^2, x_2^2, x_3^2)^T.\n",
    "$$ \n",
    "\n",
    "De la misma manera se pueden obtener clasificadores de orden tres, cuatro, cinco, etc. En general a estos clasificadores se les conoce como **clasificadores polinomiales**. Ahora, para entender bien la idea, vamos a resolver el problema anterior con un clasificador de orden 2. Sin embargo, si luego se quiere hacer el reconocimiento de otros objetos, o cambiar el orden del polinomio, pues se requeriría de reclcular cada vez la expansión polinomial a mano. Esto no es una muy buena práctica, así que vamos a generalizar la obtención de atributos polinomiales con la función `map_poly`, la cual la vamos a desarrollar a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def map_poly(grad, x):\n",
    "    \"\"\"\n",
    "    Encuentra las características polinomiales hasta el grado grad de la matriz de datos x, \n",
    "    asumiendo que x[:, 0] es la expansión de orden 0 (un vector de puros unos) y x[1:n, 0]\n",
    "    es la expansión de orden 1 (los valores de cada atributo)\n",
    "    \n",
    "    @param grad: un entero positivo con el grado de expansión\n",
    "    @param x: un ndarray de dimension (T, n + 1) donde n es el número de atributos\n",
    "    \n",
    "    @return: un ndarray de dimensión (T, n_phi + 1) donde\n",
    "             n_phi = \\sum_{i = 1}^grad fact(i + n - 1)/(fact(i) * fact(n - 1))\n",
    "    \"\"\"\n",
    "    \n",
    "    if int(grad) < 2:\n",
    "        raise ValueError('grad debe de ser mayor a 1')\n",
    "    \n",
    "    T, n = x.shape[0], x.shape[1] - 1\n",
    "    atrib = x[:,1:]\n",
    "    x_phi = x.copy()\n",
    "    for i in range(2, int(grad) + 1):\n",
    "        for comb in combinations_with_replacement(range(n), i):\n",
    "            x_phi = np.c_[x_phi, np.prod(atrib[:, comb], axis=1)]\n",
    "    return x_phi        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 6: Realiza el clasificador polinomial de orden 2 de los datos de los exámenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos a dar una guia paso a paso de lo que hay que hacer\n",
    "\n",
    "# Encuentra phi_x (x son la expansión polinomial de segundo orden, utiliza la función map_poly\n",
    "phi_x = # <--- Completa el código ----\n",
    "\n",
    "# Genera la theta inicial\n",
    "theta_phi0 = # <--- Completa el código ----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ahora hay que hacer varias pruebas para seleccionar el mejor valor de epsilon\n",
    "epsilon = # <--- Completa el código ----\n",
    "mi = 50\n",
    "_, error_hist = descenso(phi_x, y, epsilon, tol=1e-4, max_iter=mi, historial=True)\n",
    "\n",
    "plt.plot(np.arange(mi), perdida_hist)\n",
    "plt.title(r'Evolucion del valor de la $E_{in}$ en las primeras iteraciones con $\\epsilon$ = ' + str(epsilon))\n",
    "plt.xlabel('iteraciones')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Por último se hace el aprendizaje con el epsilon seleccionado\n",
    "epsilon = # <--- Completa el código ----\n",
    "w_2, _ = descenso(phi_x, y, epsilon, max_iter = int(1e6))\n",
    "print(\"Los pesos obtenidos son: \\n{}\".format(w_lotes))\n",
    "print(\"El valor final de la función de error en muestra es: {}\".format(error_muestra(w_2, phi_x, y))) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El reusltado debe de ser algo similar a:\n",
    "\n",
    "$$\n",
    "\\theta_\\phi = (15.87, -428.13, -110.51, 1.63, -0.8889, 8.6)^T\n",
    "$$\n",
    "\n",
    "Y esto lo tenemos que graficar. Pero graficar la separación de datos en una proyección en las primeras dos dimensiones, no es tan sencillo como \n",
    "lo hicimos con una separación lineal, así que vamos atener que generar un `contour`, y sobre este graficar los datos. Para esto vamos a desarrollar una función (esta la hago yo porque es un poco dificil y no da luz sobre los temas escenciales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_separacion2D(x, y, grado, theta_phi):\n",
    "    \"\"\"\n",
    "    Grafíca las primeras dos dimensiones (posiciones 1 y 2) de datos en dos dimensiones \n",
    "    extendidos con un clasificador polinomial así como la separación dada por theta_phi\n",
    "    \n",
    "    \"\"\"\n",
    "    if grado < 2:\n",
    "        raise ValueError('Esta función es para graficar separaciones con polinomios mayores a 1')\n",
    "    \n",
    "    x1_min, x1_max = np.min(x[:,1]), np.max(x[:,1])\n",
    "    x2_min, x2_max = np.min(x[:,2]), np.max(x[:,2])\n",
    "    delta1, delta2 = (x1_max - x1_min) * 0.1, (x2_max - x2_min) * 0.1\n",
    "\n",
    "    spanX1 = np.linspace(x1_min - delta1, x1_max + delta1, 200)\n",
    "    spanX2 = np.linspace(x2_min - delta2, x2_max + delta2, 200)\n",
    "    X1, X2 = np.meshgrid(spanX1, spanX2)\n",
    "        \n",
    "    Z = map_poly(grado, np.c_[np.ones((X1.size, 1)), X1.ravel(), X2.ravel()]).dot(theta_phi)\n",
    "    Z = Z.reshape(X1.shape[0], X1.shape[1])\n",
    "    \n",
    "    plt.contour(X1, X2, Z, [0], linewidths=0.5, colors='k')\n",
    "    plt.contourf(X1, X2, Z, 1, cmap=plt.cm.gray)\n",
    "    plt.plot(x[y.ravel() > 0.5, 1], x[y.ravel() > 0.5, 2], 'sr', label='aceptados')\n",
    "    plt.plot(x[y.ravel() < 0.5, 1], x[y.ravel() < 0.5, 2], 'oy', label='rechazados')\n",
    "    plt.axis([spanX1[0], spanX1[-1], spanX2[0], spanX2[-1]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a probar la función plot_separacion2D con los datos de entrenamiento. El comando tarda, ya que estamos haciendo un grid de 200$\\times$200, y realizando evaluaciones individuales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_separacion2D(x, y, 2, res.x.reshape(-1,1))\n",
    "plt.title(u\"Separación con un clasificador cuadrático\")\n",
    "plt.xlabel(u\"Calificación del primer examen\")\n",
    "plt.ylabel(u\"Calificación del segundo examen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muy bien, un clasificador polinomial de orden 2 clasifica perfectamente los datos de aprendizaje, y además parece sufucuentemente simple para ser la mejor opción para hacer la predicción.\n",
    "\n",
    "Tomemos ahora una base de datos que si bien es sintética es representativa de una familia de problemas a resolver. Supongamos que estámos opimizando la fase de pruebas dentro de la linea de producción de la empresa *Microprocesadores del Noroeste S.A. de C.V.*. La idea es reducir el banco de pruebas de cada nuevo microprocesador fabricado y en lugar de hacer 50 pruebas, redcirlas a 2. En el conjunto de datos tenemos los valores que sacó cada componente en las dos pruebas seleccionadas, y la decisión que se tomo con cada dispositivo (esta desición se tomo con el banco de 50 reglas). Los datos los podemos visualizar a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datos = np.loadtxt('datos/prod_test.txt', comments='%', delimiter=',')\n",
    "\n",
    "x, y = datos[:,0:-1], datos[:,-1:] \n",
    "x = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "\n",
    "plt.plot(x[y.ravel() == 1, 1], x[y.ravel() == 1, 2], 'sr', label='cumple calidad') \n",
    "plt.plot(x[y.ravel() == 0, 1], x[y.ravel() == 0, 2], 'ob', label='rechazado')\n",
    "plt.title(u'Ejemplo de pruebas de un producto')\n",
    "plt.xlabel(u'Valor obtenido en prueba 1')\n",
    "plt.ylabel(u'Valor obtenido en prueba 2')\n",
    "#plt.axis([20, 100, 20, 100])\n",
    "plt.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cláramente este problema no se puede solucionar con un clasificador lineal (1 orden), por lo que hay que probar otros tipos de clasificadores.\n",
    "\n",
    "#### Ejercicio 7: Prueba hacer regresión polinomial para polinomios de orden 2, 4, 6 y 8, y grafica los resultados en una figura cada uno. Recuerda que este ejercicio puede tomar bastante tiempo de cómputo dependiendo tanto del método de optimización seleccionado como de la graficacion, al final agrega una celda de texto donde comentes los resultados .\n",
    "\n",
    "Y para dar una guía de lo que se espera, anexo las imágenes de las gráficas para el polinomio de orden 2 y para el polinomio de orden 8. \n",
    "\n",
    "\n",
    "![](imagenes/ejemplo_logistica_2.png)         ![](imagenes/ejemplo_logistica_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vamos a hacer una pequeña funcion para hacer todo de forma más fácil\n",
    "\n",
    "def aprende_polinomial(x, y, grado):\n",
    "    \"\"\"\n",
    "    Aprende un clasificador polinomial para el conjunto de aprendizaje dado por x (ya extendida) y y\n",
    "    \n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "    @param y: un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    \n",
    "    @return: theta_phi un ndarray con los valores necesarios para theta extendida en el espacio de x_phi.\n",
    "    \n",
    "    \"\"\"\n",
    "    #-------------------------------------------------------------------------------------------------------\n",
    "    # Agrega tu codiggo aqui\n",
    "    #---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    \n",
    "# Ahora vamos a utilizar esto para graficar las diferentes curvas\n",
    "for grado in [2, 4, 6, 8]:\n",
    "    #------------------------------------------------\n",
    "    # agregar aqui tu código\n",
    "    #------------------------------------------------\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regularización\n",
    "\n",
    "Como podemos ver del ejercicio anterior, es dificil determinar el grado del polinomio, y en algunos casos es demasiado general (subaprendizaje) y en otros demasiado específico (sobreaprendizaje). ¿Que podría ser la solución?, bueno, una solución posible es utilizar un polinomio de alto grado (o relativamente alto), y utilizar la **regularización** para controlar la generalización del algoritmo, a través de una variable $\\lambda$.\n",
    "\n",
    "Recordemos, la función de costos de la regresión logística con regularización es:\n",
    "\n",
    "$$\n",
    "J(\\omega) = -\\frac{1}{T} \\sum_{i=1}^T \\left[ y^{(i)}\\log(\\hat{y}^{(i)}) + (1 - y^{(i)})\\log(1 - \\hat{y}^{(i)})\\right] + \\frac{\\lambda}{2T}\\sum_{j = 1}^n \\omega_j^2,\n",
    "$$\n",
    "\n",
    "que es básicamente la misma que la función de $E_{in}$, pero además se le suma el una constante por el promedio del cuadrado de los parámetros (a excepción de $\\omega_0$). Notese que en este caso lo que vamos a optimizar es una *función de costo* y no el error *en muestra*. El costo se compone de una parte que intenta hacer $E_{in}$ tienda a 0, y otra parte que intenta que el error *en muestra* se parezca (PAC) al error *fuera de muestra*.\n",
    "\n",
    "#### Ejercicio 8: Desarrolla sin utilizar ciclos `for` la forma de calcular el costo para el problema de regresión logística regularizada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def costo_regularizado(theta, x, y, lammbda):\n",
    "    \"\"\"\n",
    "    Función de costo con regularización para la regresión logística para los parámetros theta con\n",
    "    el conjunto de entrenamiento dado por x y y, y la constante de regularización lambda.\n",
    "    \n",
    "    @param theta: un ndarray de dimensión (n + 1, 1) \n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "    @param y: un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    @param lammbda: un escalar mayor o igual a 0 (o 0.0)\n",
    "    \n",
    "    @return: un flotante con el costo\n",
    "    \n",
    "    \"\"\" \n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # Inserta aquí tu código\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #----------------------------------------------------------------------------------------\n",
    "    \n",
    "# Y para probarlo vamos a hacer un unittest de los pobres, utilizando la base de datos \n",
    "# que tenemos actualmente cargada (prod_test.txt) \n",
    "\n",
    "phi_x = map_poly(6, x)\n",
    "omega_prueba = np.zeros((phi_x.shape[1], 1))\n",
    "\n",
    "assert abs(costo_regularizado(theta_prueba, phi_x, y, 1) - 0.693) <= 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, por supuesto, para poder utilizar un algoritmo eficiente de optimización, debemos ser capaces de calcular el gradiente de la función.\n",
    "\n",
    "#### Ejercicio 9: Desarrolla, aprovechando al máximo de las ventajas de ndarrays, la función del gradiente con regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradiente_regularizado(theta, x, y, lammbda):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente del costo de la regresión logística, para una theta, conociendo un conjunto de aprendizaje\n",
    "    y con regularización.\n",
    "    \n",
    "    @param theta: un ndarray de dimensión (n + 1, 1) \n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "    @param y: un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    @param lammbda: un escalar mayor o igual a 0 (o 0.0)\n",
    "    \n",
    "    @return: un ndarray de mismas dimensiones que theta\n",
    "    \n",
    "    \"\"\"\n",
    "    T = x.shape[0]\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    # Agregua aqui tu código\n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "   #------------------------------------------------------------------------\n",
    "     \n",
    "# Y para probar vamos a hacer esto:\n",
    "phi_x = map_poly(6, x)\n",
    "theta_prueba = np.zeros((phi_x.shape[1], 1))\n",
    "grad_theta_prueba = gradiente_regularizado(theta_prueba, phi_x, y, 1)\n",
    "\n",
    "assert abs(grad_theta_prueba[0] - 8.47e-3) < 1e-5\n",
    "assert abs(grad_theta_prueba[5] - 3.76e-2) < 1e-4\n",
    "assert abs(grad_theta_prueba[-1] - 3.88e-2) < 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ya por último queda probar la regresión logística con regularización, por lo que vamos a generar 3 superficies: una *aceptable*, otra con sobreaprendizaje y la última con subaprendizaje.\n",
    "\n",
    "#### Ejercicio 10: Desarrolla las funciones y scriprs necesarios para realizar la regresión logística con un polinomio de grado 8 y con regularización. Grafica la superficie de separación para $\\lambda$ igual a 1 (*aceptable*), 0 (*sobreaprendizaje*), 200 (*subaprendizaje*) .\n",
    "\n",
    "Y de nuevo, como forma de verificar si lo están haciendo bien o no, incluyo dos de las tres figuras que deben obtener, tal como yo las obtuve.\n",
    "\n",
    "![](imagenes/ejemplo_logistica_4.png)\n",
    "\n",
    "![](imagenes/ejemplo_logistica_5.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aqui pues ya no agrego nada de código para guiar el trabajo ya que es \n",
    "# bastante parecido a lo que se hizo en la sección anterior.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
